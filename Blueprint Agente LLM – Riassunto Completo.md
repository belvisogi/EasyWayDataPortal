# Blueprint Agente LLM â€“ Riassunto Completo

## Obiettivo

Costruire un **agente intelligente davanti a un LLM** (es. DeepSeek via Ollama) capace di:

* capire lâ€™intento dellâ€™utente
* riformulare correttamente lâ€™input
* scegliere cosa fare (chat, RAG, tool, workflow)
* mantenere memoria e coerenza
* migliorare lâ€™interlocuzione senza retraining del modello

> Principio chiave: **lâ€™intelligenza percepita non Ã¨ nel modello, ma nel sistema attorno al modello**.

---

## Concetti fondamentali

### LLM (Large Language Model)

* Motore statistico di generazione testo
* Non ha memoria propria
* Non conosce il contesto se non gli viene passato

### Agente

Un **agente** =

```
LLM
+ Prompt strutturati
+ Memoria esterna
+ Router / Planner
+ Tool
+ Log & feedback
```

Ãˆ lâ€™agente che:

* pulisce e interpreta lâ€™input
* decide la strategia
* orchestra le chiamate al modello

---

## Flusso generale (sempre valido)

```
Utente
  â†“
Gateway (CLI / Web / API)
  â†“
Agent Orchestrator
  â”œâ”€ Interpreter (pulizia + intento)
  â”œâ”€ Memory Manager (short / long)
  â”œâ”€ Router (decide cosa fare)
  â”œâ”€ LLM Adapter (Ollama, OpenAIâ€¦)
  â”œâ”€ Tool / RAG / Workflow
  â†“
Post-processing risposta
  â†“
Salvataggio: memoria + log + feedback
```

---

## Prompting corretto (fondamentale)

Separare sempre:

* **System Prompt**: identitÃ , lingua, tono, regole
* **Policy / Developer Prompt**: guardrail (no invenzioni, chiedi chiarimenti se serve)
* **User Prompt**: input pulito + intento

Il testo che arriva al modello **non Ã¨ mai solo quello scritto dallâ€™utente**.

---

## Memoria (non magica, ma ingegnerizzata)

### Tipi di memoria

1. **Short-term memory**

   * ultime N interazioni
   * serve per coerenza del dialogo

2. **Long-term memory**

   * preferenze utente
   * fatti utili e stabili
   * es. lingua, tono, livello tecnico

3. **Event log**

   * tutto grezzo (debug, audit, tuning)

### Storage consigliato

* Inizio: **SQLite**
* Crescita: **PostgreSQL**
* RAG: **Vector DB** (Chroma / Qdrant)

---

## Interpreter (riformulazione intelligente)

Modulo che:

* corregge typo
* espande frasi ambigue
* rileva lingua e registro
* costruisce un oggetto di intento

Esempio:

```json
{
  "intent": "framework_agente",
  "language": "it",
  "constraints": ["locale", "ollama"],
  "clean_user_message": "Quale framework usare per costruire un agente davanti a un LLM?"
}
```

---

## Router (cuore decisionale)

Decide **come** rispondere:

* **CHAT** â†’ conversazione pura
* **RAG** â†’ risposta basata su documenti
* **TOOL** â†’ azioni (DB, file, API)
* **WORKFLOW** â†’ task multi-step

Regole tipiche:

* â€œsecondo i miei documentiâ€ â†’ RAG
* â€œfai / esegui / aggiornaâ€ â†’ TOOL
* â€œprocedura / pipeline / piÃ¹ stepâ€ â†’ WORKFLOW
* altrimenti â†’ CHAT

---

## Blueprint per casi dâ€™uso

### 1ï¸âƒ£ Chat Agent

* memoria
* stile coerente
* nessun tool

### 2ï¸âƒ£ RAG Agent

* ingestion documenti
* retrieval + context composer
* risposta *grounded*
* se info non trovata â†’ lo dice

### 3ï¸âƒ£ Tool Agent

* registry tool con schema
* plan â†’ execute â†’ summarize
* policy di sicurezza

### 4ï¸âƒ£ Workflow Agent

* state machine
* step, retry, checkpoint
* sotto-agenti con ruoli

### 5ï¸âƒ£ Hybrid Agent (finale)

* un unico orchestrator
* router centrale
* governa tutto

---

## Ciclo di miglioramento (senza retraining)

1. log input/output
2. feedback (ðŸ‘ / ðŸ‘Ž)
3. analisi errori
4. tuning di:

   * prompt
   * router
   * memoria

> Lâ€™agente **migliora** anche se il modello resta identico.

---

## Framework consigliati

* **LangChain (Python/JS)** â€“ rapido, completo
* **LlamaIndex (Python)** â€“ eccellente per RAG
* **Semantic Kernel** â€“ enterprise / plugin-based
* **AutoGen / CrewAI** â€“ multi-agente
* **Custom Agent (consigliato)** â€“ massimo controllo

Con Ollama + DeepSeek: **LangChain o custom loop**.

---

## Struttura progetto consigliata

```
agent/
  app.py
  orchestrator.py
  router.py
  interpreter.py
  llm/
    ollama_adapter.py
  memory/
    short_term.py
    long_term.py
    store.sqlite
  rag/
    ingest.py
    retriever.py
  tools/
    registry.py
    runner.py
  workflows/
    engine.py
  observability/
    logger.py
    feedback.py
```

---

## Loop centrale (pseudo-codice)

```python
def handle_message(user_text):
    parsed = interpreter(user_text)
    ctx = memory.load_short_term()
    profile = memory.load_long_term()

    route = router.decide(parsed, ctx, profile)

    if route == "rag":
        evidence = rag.retrieve(parsed)
        answer = llm.answer(ctx, profile, parsed, evidence)

    elif route == "tool":
        plan = llm.plan_tools(ctx, profile, parsed)
        results = tools.execute(plan)
        answer = llm.summarize(results)

    elif route == "workflow":
        answer = workflows.run(parsed)

    else:
        answer = llm.chat(ctx, profile, parsed)

    memory.save_turn(user_text, answer)
    observability.log(parsed, route, answer)
    return answer
```

---

## Roadmap consigliata

1. Chat agent stabile
2. Router
3. RAG
4. Tool agent
5. Workflow agent

---

## Frase finale da ricordare

> **Il modello genera testo. Lâ€™agente genera intelligenza.**

Questo documento Ã¨ la base per costruire un agente LLM serio, locale, estendibile e governabile.
